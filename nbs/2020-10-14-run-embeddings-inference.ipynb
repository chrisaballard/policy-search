{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab98357-9a29-47e2-9119-c825c42c4a08",
   "metadata": {},
   "source": [
    "# Running embeddings inference\n",
    "\n",
    "This notebook produces `sBERT` embeddings and sector and instrument classifications (based on cosine distance) for text in a dataset. It saves these predictions in batches and then loads them to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b8df39-526b-4007-b569-6abb3bdea25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: torch in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (1.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (4.62.3)\n",
      "Requirement already satisfied: boto3 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (1.18.61)\n",
      "Requirement already satisfied: fsspec in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (2021.10.0)\n",
      "Requirement already satisfied: s3fs in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already satisfied: scipy in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (1.0)\n",
      "Requirement already satisfied: numpy in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: nltk in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (3.6.5)\n",
      "Requirement already satisfied: sentencepiece in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: torchvision in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (4.11.3)\n",
      "Requirement already satisfied: huggingface-hub in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (0.0.19)\n",
      "Requirement already satisfied: typing-extensions in /Users/kalyan/.local/lib/python3.8/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from boto3) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.61 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from boto3) (1.21.61)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/kalyan/.local/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.61->boto3) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.61->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.61->boto3) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kalyan/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: requests in /Users/kalyan/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/kalyan/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/kalyan/.local/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kalyan/.local/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /Users/kalyan/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from torchvision->sentence-transformers) (8.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers torch tqdm boto3 fsspec s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f369eeac-2780-425b-86f6-86a67a67e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Callable\n",
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as sbert_utils\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import boto3\n",
    "\n",
    "from weak_sentence_classification.utils import Schema, CosineDistanceClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95378ab5-306b-4c1c-be67-c96189623f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = [\n",
    "    {\n",
    "        \"model_name\": \"multi-qa-MiniLM-L6-cos-v1\",\n",
    "        \"distance_measure\": \"cosine\",\n",
    "        \"predict_sectors_instruments\": True,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"msmarco-distilbert-dot-v5\",\n",
    "        \"distance_measure\": \"dot_product\",\n",
    "        \"predict_sectors_instruments\": False,\n",
    "    },\n",
    "]\n",
    "\n",
    "S3_BUCKET_URL = 's3://cpr-policy-bucket/'\n",
    "DATASET_FILENAME = 'policy_dataset.csv.gz'\n",
    "\n",
    "LOCAL_OUTPUT_PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379334d-0dda-4bb9-a898-5796e7dd2824",
   "metadata": {},
   "source": [
    "## 1. Load data and set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef192a6c-c3fa-483c-a542-60d01ffbaacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1666918 entries, 0 to 1666917\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   Unnamed: 0   1666918 non-null  int64 \n",
      " 1   policy_id    1666918 non-null  int64 \n",
      " 2   policy_name  1666918 non-null  object\n",
      " 3   page_id      1666918 non-null  int64 \n",
      " 4   text         1666918 non-null  object\n",
      " 5   text_id      1666918 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 76.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# s3_dataset_path = S3_BUCKET_URL + DATASET_FILENAME\n",
    "s3_dataset_path = \"../../s3-buckets/cpr-datasets/policy_dataset.csv.gz\"\n",
    "df = pd.read_csv(s3_dataset_path)\n",
    "\n",
    "# Add unique id for each text entry\n",
    "df['text_id'] = range(0, len(df))\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3edf4c-a70b-4515-893a-a47c001b5380",
   "metadata": {},
   "source": [
    "### 1.1 Filter dataset by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce84cb3-944f-49c9-9b1b-779fc524c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure whether this is the right thing to do here, because we need all embeddings for search. TODO: have a think about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b967e-e74f-4aed-9643-9aab5bce8e5b",
   "metadata": {},
   "source": [
    "### 1.2 Get sector and instrument schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb24a72a-255a-4e92-a5b0-489296187b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_FOLDER = Path(\"../schema\")\n",
    "\n",
    "instruments = Schema.from_yaml_path(SCHEMA_FOLDER/\"instruments.yml\")\n",
    "sectors = Schema.from_yaml_path(SCHEMA_FOLDER/\"sectors.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04338b79-01aa-4f84-93cd-422885c74973",
   "metadata": {},
   "source": [
    "### 1.3 Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9916658-007a-4323-af90-6ec642a0a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, config in enumerate(MODEL_CONFIG):\n",
    "    MODEL_CONFIG[idx][\"sbert_model\"] = SentenceTransformer(config[\"model_name\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e0f91-4685-4b4d-9c1b-53ce2f8f2878",
   "metadata": {},
   "source": [
    "## 2. Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45a01459-225e-4f57-920a-7747d0f1b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_batches(text_arr, batch_size):\n",
    "    n = len(text_arr)\n",
    "    batch_total = math.ceil(n / batch_size)\n",
    "    \n",
    "    return batch_total\n",
    "\n",
    "def get_batches(text_arr, batch_size):\n",
    "    n = len(text_arr)\n",
    "    batch_total_idx = math.ceil(n / batch_size) * batch_size\n",
    "    for batch_idx in range(0, batch_total_idx, batch_size):\n",
    "        yield text_arr[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "def save_embeddings(batches_query_emb, save_path: Path):\n",
    "    \"\"\"Save compressed embeddings\"\"\"\n",
    "\n",
    "    emb = torch.cat(batches_query_emb)\n",
    "    \n",
    "    file = gzip.GzipFile(save_path, 'wb')\n",
    "    file.write(pickle.dumps(emb, protocol=pickle.HIGHEST_PROTOCOL))\n",
    "    file.close()\n",
    "    \n",
    "def load_embeddings(filename):\n",
    "    \"\"\"Load compressed embeddings\n",
    "    \"\"\"\n",
    "    file = gzip.GzipFile(filename, 'rb')\n",
    "    data = file.read()\n",
    "    obj = pickle.loads(data)\n",
    "    file.close()\n",
    "    return obj\n",
    "        \n",
    "def normalise_predictions(b_df, preds, conf, schema):\n",
    "    b_preds = []\n",
    "    for ix in range(len(b_df)):\n",
    "        for p_ix, p in enumerate(preds[ix]):\n",
    "            b_preds.append(\n",
    "                {\n",
    "                    'policy_id': b_df.iloc[ix]['policy_id'],\n",
    "                    'page_id': b_df.iloc[ix]['page_id'],\n",
    "                    'text_id': b_df.iloc[ix]['text_id'],\n",
    "                    'schema': schema,\n",
    "                    'pred': p,\n",
    "                    'conf': conf[ix][p_ix]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(b_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3be93529-6e4b-499b-a1ad-8b19a0018f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-qa-MiniLM-L6-cos-v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f3f4eb495e4f56bcdc2bd3b7564ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running msmarco-distilbert-dot-v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c44be193d441e093ed426c4de56022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, config in enumerate(MODEL_CONFIG):\n",
    "    print(f\"Running {config['model_name']}\")\n",
    "    \n",
    "    embedding_path = LOCAL_OUTPUT_PATH / f\"policy_text_embeddings_{config['model_name']}.pkl.gz\"\n",
    "    predictions_path = LOCAL_OUTPUT_PATH / f\"policy_text_predictions_{config['model_name']}.csv.gz\"\n",
    "\n",
    "    MODEL_CONFIG[idx][\"embedding_path\"] = embedding_path\n",
    "    MODEL_CONFIG[idx][\"predictions_path\"] = predictions_path\n",
    "    \n",
    "    if predictions_path.exists():\n",
    "        predictions_path.unlink()\n",
    "\n",
    "    threshold = 0.35\n",
    "    save_every = 1\n",
    "    reset_batches = False\n",
    "    batch_size = 100\n",
    "\n",
    "    n_batches = total_batches(df, batch_size)\n",
    "\n",
    "    instrument_clf = CosineDistanceClassifier(\n",
    "        instruments, \n",
    "        sbert_model= config[\"sbert_model\"], \n",
    "        distance_measure= config[\"distance_measure\"],\n",
    "        concat_keywords_with_subsectors=True\n",
    "    )\n",
    "    sector_clf = CosineDistanceClassifier(\n",
    "        sectors, \n",
    "        sbert_model= config[\"sbert_model\"], \n",
    "        distance_measure= config[\"distance_measure\"],\n",
    "        concat_keywords_with_subsectors=True\n",
    "    )\n",
    "\n",
    "\n",
    "    all_query_emb = []\n",
    "    all_instr_preds = []\n",
    "    all_instr_conf = []\n",
    "    all_sector_preds = []\n",
    "    all_sector_conf = []\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for b_ix, b_df in enumerate(tqdm(get_batches(df, batch_size), total=n_batches)):\n",
    "        batch_query_emb, instrument_preds, instrument_conf = instrument_clf.predict(b_df.text.values, threshold, True)\n",
    "        sector_preds, sector_conf = sector_clf.predict(b_df.text.values, threshold, False)\n",
    "\n",
    "        all_query_emb.append(batch_query_emb)\n",
    "\n",
    "        if config[\"predict_sectors_instruments\"]:\n",
    "            predictions = pd.concat(\n",
    "                [\n",
    "                    normalise_predictions(b_df, instrument_preds, instrument_conf, 'instrument'),\n",
    "                    normalise_predictions(b_df, sector_preds, sector_conf, 'sector')\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if b_ix % save_every == 0:\n",
    "                if predictions_path.exists():\n",
    "                    mode = 'at'\n",
    "                    header = False\n",
    "                else:\n",
    "                    mode = 'wt'\n",
    "                    header = True\n",
    "                predictions.to_csv(predictions_path, mode=mode, header=header, index=False), \n",
    "\n",
    "\n",
    "    save_embeddings(all_query_emb, embedding_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7799e6-3503-49d4-a2f2-cac9bc1b2bd5",
   "metadata": {},
   "source": [
    "## 3. Save predictions and embeddings to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983f42c-643a-4608-8053-7f46aaa55c06",
   "metadata": {},
   "source": [
    "### 3.1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba380e7e-05c8-4116-b735-fb69e9d7f7df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Filename must be a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nv/ml84yvj11ns_bxrq3c40q2xc0000gn/T/ipykernel_57479/1563027046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ms3_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ms3_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpr-datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_sectors_instruments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mS3Transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         return transfer.upload_file(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             extra_args=ExtraArgs, callback=Callback)\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Filename must be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0msubscribers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_subscribers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Filename must be a string"
     ]
    }
   ],
   "source": [
    "for config in MODEL_CONFIG:       \n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(config['embedding_path'], 'cpr-datasets', config['embedding_path'].name)\n",
    "    \n",
    "    if config['predict_sectors_instruments']:\n",
    "        s3_client.upload_file(config['predictions_path'], 'cpr-datasets', config['predictions_path'].name)\n",
    "        \n",
    "    print(f\"{config['model_name']} uploaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c970127-9486-438f-b9f1-8561141c8d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
